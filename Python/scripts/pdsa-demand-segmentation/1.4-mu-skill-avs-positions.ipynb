{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d675d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 freeze > \"../requirements.txt\"\n",
    "#!pip3 install -r \"../requirements.txt\"  # giving some error\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import copy\n",
    "import copy\n",
    "import pickle\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from pins import board_rsconnect\n",
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "from datetime import timedelta\n",
    "import pytz\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500) \n",
    "\n",
    "def GBQ_data(query_string):\n",
    "    client = bigquery.Client('turing-230020')\n",
    "    query = client.query(query_string)\n",
    "    results = query.result()\n",
    "    return results.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2bbe6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5572f040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22a01a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9593, 33)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "API_KEY = 'yDAssnMUtqatxoOpyNgYVKZcXfLP3vwD' \n",
    "SERVER = 'https://rstudio-connect.turing.com/'\n",
    "board = board_rsconnect(server_url=SERVER, api_key=API_KEY)\n",
    "job_data =board.pin_read(\"muhammad_usman/demand-basic-data\")\n",
    "print(job_data.shape)\n",
    "\n",
    "job_data_cols = ['job_id', 'company', 'client_category','opportunity_created_date', 'dc_date', 'max_acceptable_rate', 'is_deleted',\n",
    "                 'job_value', 'status', 'num_must_have_skills', 'must_have_total_years_of_experience',\n",
    "                'ms_num_queries', 'ss_queries', 'queries', 'interviews_requested', 'interviews_scheduled',\n",
    "                'interviews_happened','interviews_passed', 'interviews_failed', 'region', 'engagement_days',\n",
    "                'number_of_open_roles', 'total_positions', 'total_open_positions__c', 'must_have_skill_ids', 'must_have_skill_names']\n",
    "\n",
    "final_data = job_data[job_data_cols].copy()\n",
    "final_data['opportunity_created_date'] = pd.to_datetime(final_data['opportunity_created_date'])\n",
    "final_data['dc_date'] = pd.to_datetime(final_data['dc_date'])\n",
    "final_data['dc'] = np.where(final_data['dc_date'].isna(), 0,1)\n",
    "final_data['ninety_days_old'] = np.where((final_data['opportunity_created_date'] > dt.datetime.now(pytz.utc) - timedelta(days = 90)) & (final_data['dc']==0) , 1,0)\n",
    "final_data['time_taken_DC'] = (final_data['dc_date'] - final_data['opportunity_created_date']).dt.days\n",
    "#final_data['time_taken_DC'] = np.where(final_data['time_taken_DC']<0, 0, final_data['time_taken_DC'])\n",
    "final_data['enterprise'] = np.where(final_data['client_category']=='1.Platinum', 1,0)\n",
    "final_data['active'] = np.where(~(final_data['status'].isin(['Lost Opportunity', 'Project Aborted', 'Paused'])) & (final_data['dc']==0), 1,0)\n",
    "final_data['active_roles'] = np.where(final_data['active']==1, final_data['total_positions'],0)\n",
    "final_data['lost'] = np.where((final_data['status'].isin(['Lost Opportunity', 'Project Aborted', 'Paused']) & (final_data['dc']==0)), 1,0)\n",
    "final_data['max_acceptable_rate'] = np.where(final_data['max_acceptable_rate'] >300 , 300, final_data['max_acceptable_rate'])\n",
    "final_data.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2e4f0b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7943, 30)\n",
      "(7913, 14)\n"
     ]
    }
   ],
   "source": [
    "pos = \"\"\"with pos_data as(SELECT * from(\n",
    "SELECT *, row_number() over(partition by job_id order by opportunity_created_date desc) as rn\n",
    "from `matchingmetrics.opps_positions` where opportunity_type='parent') where rn=1)\n",
    "\n",
    "SELECT * from pos_data\n",
    "left join turing-dev-337819.pdsa.demand_cluster using(job_id)\n",
    "\"\"\"\n",
    "\n",
    "pos_data = GBQ_data(pos)\n",
    "print(pos_data.shape)\n",
    "pos_data_cols = ['job_id', 'account', 'opportunity_name', 'is_platinum', 'total_positions', 'chosen_sum_in_bulk_job', 'starts_sum_in_bulk_job','client_category', 'cluster']\n",
    "pos_data = pos_data[pos_data_cols].copy()\n",
    "\n",
    "pos_data = pos_data.loc[pos_data['account']!='OpenAI'].copy()\n",
    "pos_data = pos_data.merge(final_data[['job_id', 'active', 'lost', 'num_must_have_skills']], how='left', on='job_id')\n",
    "pos_data['active_positions'] = np.where(pos_data['lost']==0, pos_data['total_positions']-pos_data['chosen_sum_in_bulk_job'], 0)\n",
    "print(pos_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4b4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4f94dfc",
   "metadata": {},
   "source": [
    "## Combination of Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f6b3e19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'yDAssnMUtqatxoOpyNgYVKZcXfLP3vwD' \n",
    "SERVER = 'https://rstudio-connect.turing.com/'\n",
    "board = board_rsconnect(server_url=SERVER, api_key=API_KEY)\n",
    "job_all_data = board.pin_read(\"muhammad_usman/demand-avs-data\")\n",
    "job_all_data = job_all_data.loc[~job_all_data['must_have_skill_ids_tuple'].isna()]\n",
    "\n",
    "\n",
    "job_mcq_data = job_all_data[['job_id', 'must_have_skill_ids', 'must_have_skill_names', 'must_have_skill_ids_tuple', 'passed_must_have_skill total_devs',\n",
    "                                                     'passed_must_have_skill Elite',\n",
    "                                                      'passed_must_have_skill High Quality',\n",
    "                                                      'passed_must_have_skill Average',\n",
    "                                                      'passed_must_have_skill Low Quality',\n",
    "                                                      'passed_must_have_skill Low Experience']]\n",
    "\n",
    "merge_data_pos_mcq = pos_data.merge(job_mcq_data, how='left', on='job_id')\n",
    "merge_data_pos_mcq = merge_data_pos_mcq.loc[~merge_data_pos_mcq['num_must_have_skills'].isna()]\n",
    "\n",
    "duck1_comb = merge_data_pos_mcq.groupby(['must_have_skill_ids_tuple','must_have_skill_names']).agg({'num_must_have_skills':'mean',\n",
    "                                                      'total_positions':'sum', 'chosen_sum_in_bulk_job':'sum',\n",
    "                                                      'active_positions':'sum',\n",
    "                                                      'passed_must_have_skill total_devs':'mean',\n",
    "                                                      'passed_must_have_skill Elite':'mean',\n",
    "                                                      'passed_must_have_skill High Quality':'mean',\n",
    "                                                      'passed_must_have_skill Average':'mean',\n",
    "                                                      'passed_must_have_skill Low Quality':'mean',\n",
    "                                                      'passed_must_have_skill Low Experience':'mean'}).reset_index()#.to_csv('job_all_data.csv')\n",
    "duck1_comb.sort_values(by='total_positions', ascending=False)\n",
    "duck1_comb['sk_li'] = duck1_comb.must_have_skill_ids_tuple.apply(lambda x:x.replace('(', '').replace(')','')).apply(lambda x:x.split(','),-1)\n",
    "duck1_comb['single_skill'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039f24e",
   "metadata": {},
   "source": [
    "## DATA Duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ab9553dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_data = pd.DataFrame()\n",
    "for x in duck1_comb.index:\n",
    "    n = int(duck1_comb.at[x,'num_must_have_skills']) #len(duck1_comb.at[x,'sk_li'])\n",
    "    sk_list = duck1_comb.at[x,'sk_li']\n",
    "    temp_data = pd.DataFrame(duck1_comb.loc[x,]).transpose()\n",
    "    temp_data = pd.concat([temp_data]*n, ignore_index=True)\n",
    "    for y in temp_data.index:\n",
    "        temp_data.at[y, 'single_skill'] = sk_list[y]\n",
    "    global_data = global_data.append(temp_data, ignore_index=True)\n",
    "skill_map = GBQ_data(\"\"\"SELECT id,skill_name FROM `turing-230020.devdb_mirror.base_all_skills_v4`\"\"\")\n",
    "global_data['single_skill'] = global_data.single_skill.apply(lambda x:int(x))\n",
    "global_data = global_data.merge(skill_map, how='left', left_on='single_skill', right_on='id')\n",
    "global_data.to_csv('combination_of_skills.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315925af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3d4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "324539fe",
   "metadata": {},
   "source": [
    "## Single Skill\n",
    "- If a job has 2 skills (React, Python) then that job will be considered for both skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "11cf0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_mcq_must = \"\"\"\n",
    "SELECT \n",
    "    job.job_id, \n",
    "    job.skill_id, \n",
    "    skill.skill_name\n",
    "from `turing-230020.devdb_mirror.ms2_job_skill` job\n",
    "LEFT join `turing-230020.devdb_mirror.base_all_skills_v4` skill on job.skill_id=skill.id\n",
    "where job.job_skill_level_id=1\n",
    "\"\"\"\n",
    "\n",
    "job_mcq_must = GBQ_data(job_mcq_must)\n",
    "\n",
    "\n",
    "ff = pos_data.merge(job_mcq_must, how='left', on='job_id')\n",
    "ff = ff.loc[~ff['num_must_have_skills'].isna()]\n",
    "ff = ff.groupby(['skill_id', 'skill_name']).agg({'total_positions':'sum','chosen_sum_in_bulk_job':'sum','active_positions':'sum'}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac7d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3e5b8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "API_KEY = 'yDAssnMUtqatxoOpyNgYVKZcXfLP3vwD' \n",
    "SERVER = 'https://rstudio-connect.turing.com/'\n",
    "board = board_rsconnect(server_url=SERVER, api_key=API_KEY)\n",
    "dev_data = board.pin_read(\"muhammad_usman/devs-skill-pass-list\")\n",
    "dev_data['last_update_availability'] = pd.to_datetime(dev_data['last_update_availability'])\n",
    "dev_data = dev_data.loc[(dev_data['num_passed_skill']>0) & (~dev_data['num_skill'].isna())]\n",
    "dev_data = dev_data.loc[(dev_data['vetting_status'].isin(['available supply','ready 2.0','passed ti','standardized resume']))].reset_index(drop=True).copy()\n",
    "dev_data['self_skill_id_list'] = dev_data.self_skill_id.apply(lambda x:list(map(int, x.split(',',-1))))\n",
    "dev_data['passed_skill_id_list'] = dev_data.passed_skill_id.apply(lambda x:list(map(int, x.replace('[', '').replace(']','').split(',',-1))))\n",
    "dev_data['passed_skill_id_set'] = dev_data.passed_skill_id_list.apply(lambda x:set(x))\n",
    "dev_data['self_skill_id_set'] = dev_data.self_skill_id_list.apply(lambda x:set(x))\n",
    "dev_data.shape\n",
    "\n",
    "def avs_supply_passed_mcq(skill):\n",
    "    temp_data = dev_data                            \n",
    "    for s in skill:\n",
    "        temp_data = temp_data.loc[temp_data['passed_skill_id_set'].apply(lambda x:s in x)]\n",
    "    return temp_data['cluster_label'].value_counts().append(pd.Series(temp_data.shape[0], index = ['total_devs']))\n",
    "\n",
    "col_name = ['total_devs', 'Elite', 'High Quality', 'Average', 'Low Quality', 'Low Experience']\n",
    "df_passed = pd.DataFrame(columns = col_name)\n",
    "for i in ff.skill_id:\n",
    "    df_passed = df_passed.append(avs_supply_passed_mcq([i]),ignore_index=True)\n",
    "\n",
    "df_passed['skill_id'] = ff.skill_id\n",
    "ff.merge(df_passed, how='left', on='skill_id').to_csv('without_openai_active.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b2cf0667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff.merge(df_passed, how='left', on='skill_id').sort_values(by='total_positions', ascending=False)\n",
    "#ff.merge(df_passed, how='left', on='skill_id').isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd80296f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a33b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83ae2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b441f15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
