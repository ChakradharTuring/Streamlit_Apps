{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36dc0430",
   "metadata": {},
   "source": [
    "## Import data processed data for new devs and predict label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1fddb4",
   "metadata": {},
   "source": [
    "## Imports and global declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbee2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze > \"../requirements.txt\"\n",
    "#!pip3 install -r \"../requirements.txt\"  # giving some error\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import copy\n",
    "import copy\n",
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe74b2",
   "metadata": {},
   "source": [
    "## Load process data from csv 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e19a99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prediction csv file from ../data/processed/1.3-um-data-process-predict-08-08-22.csv\n"
     ]
    }
   ],
   "source": [
    "# . is any character except new line, he.{2} all should match where we have 2 characters after he\n",
    "# .*he mean any number of character before he\n",
    "a_predict = [re.search(r'\\d{2}-\\d{2}-\\d{2}', x).group(0) for x in glob.glob(\"../data/processed/1.3*\")]\n",
    "b_predict = sorted([dt.datetime.strptime(x,\"%d-%m-%y\") for x in a_predict])\n",
    "file_path = [val for val in glob.glob(\"../data/processed/1.3*\") if re.match('.*' + b_predict[-1].strftime('%d-%m-%y'),val)][0]\n",
    "print(f\"Loading prediction csv file from {file_path}\")\n",
    "predict_data = pd.read_csv(file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5b826",
   "metadata": {},
   "source": [
    "## Loading knn clustering model and performing prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "113f7d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering model loaded from ../models/2.0-knn-clustering-model-20-07-22.sav\n"
     ]
    }
   ],
   "source": [
    "a = [re.search(r'\\d{2}-\\d{2}-\\d{2}', x).group(0) for x in glob.glob(\"../models/2.0-knn-clustering-model*\")]\n",
    "b = sorted([dt.datetime.strptime(x,\"%d-%m-%y\") for x in a])\n",
    "clustering_model_path = [val for val in glob.glob(\"../models/2.0-knn-clustering-model*\") if re.match('.*' + b[-1].strftime('%d-%m-%y'),val)][0]\n",
    "print(f\"Clustering model loaded from {clustering_model_path}\")\n",
    "clustering_model = pickle.load(open(clustering_model_path, 'rb'))\n",
    "predict_data['cluster'] = clustering_model.predict(predict_data.loc[:, ~predict_data.columns.isin(['dev_id', 'paying_cust'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528216c",
   "metadata": {},
   "source": [
    "## Loading cluster lable and perform mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e194449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster label loaded from ../models/2.0-knn-cluster-label-20-07-22.pckl\n"
     ]
    }
   ],
   "source": [
    "a = [re.search(r'\\d{2}-\\d{2}-\\d{2}', x).group(0) for x in glob.glob(\"../models/2.0-knn-cluster-label*\")]\n",
    "b = sorted([dt.datetime.strptime(x,\"%d-%m-%y\") for x in a])\n",
    "cluster_label_path = [val for val in glob.glob(\"../models/2.0-knn-cluster-label*\") if re.match('.*' + b[-1].strftime('%d-%m-%y'),val)][0]\n",
    "print(f\"Cluster label loaded from {cluster_label_path}\")\n",
    "cluster_label = pickle.load(open(cluster_label_path, 'rb'))\n",
    "predict_data['label'] = predict_data['cluster'].map(cluster_label)\n",
    "label_id_correction_old_model = {'Elite':4, 'High Quality': 2,  'Average': 1, 'Low Quality':3,'Low Experience':0}\n",
    "predict_data['cluster'] = predict_data['label'].map(label_id_correction_old_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7231801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of predicted cluster\n",
      "   cluster           label  Number of devs\n",
      "0        0  Low Experience             226\n",
      "1        1         Average             149\n",
      "2        2    High Quality              68\n",
      "3        3     Low Quality             115\n",
      "4        4           Elite               4\n"
     ]
    }
   ],
   "source": [
    "print('Distribution of predicted cluster')\n",
    "print(predict_data.groupby(['cluster', 'label']).agg({'dev_id':'nunique'}).reset_index().rename(columns={'dev_id':'Number of devs'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cac0935",
   "metadata": {},
   "source": [
    "## Pushing cluster lable to GBQ table for new devs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe52faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1260.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster label for new devs pushed in GBQ table with shape (562, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "v2_cols = ['dev_id', 'cluster', 'cluster_label', 'date_created']\n",
    "df = predict_data[['dev_id', 'cluster', 'label']].copy()\n",
    "df.rename(columns={'label':'cluster_label'},inplace=True)\n",
    "df['date_created'] = dt.datetime.now(timezone.utc)\n",
    "\n",
    "if df['dev_id'].duplicated().any():\n",
    "    print('label data has duplicated dev_id')\n",
    "else:\n",
    "    pandas_gbq.to_gbq(df, 'pdsa.PDAS_P2_cluster', project_id='turing-dev-337819', if_exists='append')\n",
    "    print(f\"Cluster label for new devs pushed in GBQ table with shape {df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
