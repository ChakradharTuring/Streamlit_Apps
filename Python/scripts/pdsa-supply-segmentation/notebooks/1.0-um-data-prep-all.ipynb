{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "162fc801",
   "metadata": {},
   "source": [
    "# Import data from google big query and store in local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39048d24",
   "metadata": {},
   "source": [
    "## Imports and global declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d284564",
   "metadata": {},
   "outputs": [
    {
     "ename": "DistributionNotFound",
     "evalue": "The 'google-cloud-bigquery-storage' distribution was not found and is required by the application",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDistributionNotFound\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/muhammad.usman/testuser3/pdsa-tasks/pdsa-supply-segmentation/notebooks/1.0-um-data-prep-all.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://workbench.turing.com/home/muhammad.usman/testuser3/pdsa-tasks/pdsa-supply-segmentation/notebooks/1.0-um-data-prep-all.ipynb#ch0000002vscode-remote?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip3 freeze > \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../requirements.txt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://workbench.turing.com/home/muhammad.usman/testuser3/pdsa-tasks/pdsa-supply-segmentation/notebooks/1.0-um-data-prep-all.ipynb#ch0000002vscode-remote?line=1'>2</a>\u001b[0m \u001b[39m#!pip3 install -r \"../requirements.txt\"  # giving some error\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://workbench.turing.com/home/muhammad.usman/testuser3/pdsa-tasks/pdsa-supply-segmentation/notebooks/1.0-um-data-prep-all.ipynb#ch0000002vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m \u001b[39mimport\u001b[39;00m bigquery\n\u001b[1;32m      <a href='vscode-notebook-cell://workbench.turing.com/home/muhammad.usman/testuser3/pdsa-tasks/pdsa-supply-segmentation/notebooks/1.0-um-data-prep-all.ipynb#ch0000002vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://workbench.turing.com/home/muhammad.usman/testuser3/pdsa-tasks/pdsa-supply-segmentation/notebooks/1.0-um-data-prep-all.ipynb#ch0000002vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/google/cloud/bigquery/__init__.py:35\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery/__init__.py?line=30'>31</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbigquery\u001b[39;00m \u001b[39mimport\u001b[39;00m version \u001b[39mas\u001b[39;00m bigquery_version\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery/__init__.py?line=32'>33</a>\u001b[0m __version__ \u001b[39m=\u001b[39m bigquery_version\u001b[39m.\u001b[39m__version__\n\u001b[0;32m---> <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery/__init__.py?line=34'>35</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbigquery\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m Client\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery/__init__.py?line=35'>36</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbigquery\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m AccessEntry\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery/__init__.py?line=36'>37</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbigquery\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/google/cloud/bigquery/client.py:59\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery/client.py?line=56'>57</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m \u001b[39mimport\u001b[39;00m exceptions  \u001b[39m# pytype: disable=import-error\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery/client.py?line=57'>58</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m ClientWithProject  \u001b[39m# type: ignore  # pytype: disable=import-error\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery/client.py?line=58'>59</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbigquery_storage_v1\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservices\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbig_query_read\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery/client.py?line=59'>60</a>\u001b[0m     DEFAULT_CLIENT_INFO \u001b[39mas\u001b[39;00m DEFAULT_BQSTORAGE_CLIENT_INFO,\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery/client.py?line=60'>61</a>\u001b[0m )\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery/client.py?line=62'>63</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbigquery\u001b[39;00m \u001b[39mimport\u001b[39;00m _job_helpers\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery/client.py?line=63'>64</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbigquery\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_job_helpers\u001b[39;00m \u001b[39mimport\u001b[39;00m make_job_id \u001b[39mas\u001b[39;00m _make_job_id\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/google/cloud/bigquery_storage_v1/__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery_storage_v1/__init__.py?line=16'>17</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m absolute_import\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery_storage_v1/__init__.py?line=18'>19</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpkg_resources\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery_storage_v1/__init__.py?line=20'>21</a>\u001b[0m __version__ \u001b[39m=\u001b[39m pkg_resources\u001b[39m.\u001b[39;49mget_distribution(\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery_storage_v1/__init__.py?line=21'>22</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mgoogle-cloud-bigquery-storage\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery_storage_v1/__init__.py?line=22'>23</a>\u001b[0m )\u001b[39m.\u001b[39mversion  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery_storage_v1/__init__.py?line=24'>25</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbigquery_storage_v1\u001b[39;00m \u001b[39mimport\u001b[39;00m client\n\u001b[1;32m     <a href='file:///home/muhammad.usman/.local/lib/python3.9/site-packages/google/cloud/bigquery_storage_v1/__init__.py?line=25'>26</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcloud\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbigquery_storage_v1\u001b[39;00m \u001b[39mimport\u001b[39;00m types\n",
      "File \u001b[0;32m/opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py:477\u001b[0m, in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=474'>475</a>\u001b[0m     dist \u001b[39m=\u001b[39m Requirement\u001b[39m.\u001b[39mparse(dist)\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=475'>476</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dist, Requirement):\n\u001b[0;32m--> <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=476'>477</a>\u001b[0m     dist \u001b[39m=\u001b[39m get_provider(dist)\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=477'>478</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dist, Distribution):\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=478'>479</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected string, Requirement, or Distribution\u001b[39m\u001b[39m\"\u001b[39m, dist)\n",
      "File \u001b[0;32m/opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py:353\u001b[0m, in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=350'>351</a>\u001b[0m \u001b[39m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=351'>352</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(moduleOrReq, Requirement):\n\u001b[0;32m--> <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=352'>353</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m working_set\u001b[39m.\u001b[39mfind(moduleOrReq) \u001b[39mor\u001b[39;00m require(\u001b[39mstr\u001b[39;49m(moduleOrReq))[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=353'>354</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=354'>355</a>\u001b[0m     module \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules[moduleOrReq]\n",
      "File \u001b[0;32m/opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py:897\u001b[0m, in \u001b[0;36mWorkingSet.require\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=887'>888</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequire\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mrequirements):\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=888'>889</a>\u001b[0m     \u001b[39m\"\"\"Ensure that distributions matching `requirements` are activated\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=889'>890</a>\u001b[0m \n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=890'>891</a>\u001b[0m \u001b[39m    `requirements` must be a string or a (possibly-nested) sequence\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=894'>895</a>\u001b[0m \u001b[39m    included, even if they were already activated in this working set.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=895'>896</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=896'>897</a>\u001b[0m     needed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresolve(parse_requirements(requirements))\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=898'>899</a>\u001b[0m     \u001b[39mfor\u001b[39;00m dist \u001b[39min\u001b[39;00m needed:\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=899'>900</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd(dist)\n",
      "File \u001b[0;32m/opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py:783\u001b[0m, in \u001b[0;36mWorkingSet.resolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=780'>781</a>\u001b[0m         \u001b[39mif\u001b[39;00m dist \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=781'>782</a>\u001b[0m             requirers \u001b[39m=\u001b[39m required_by\u001b[39m.\u001b[39mget(req, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=782'>783</a>\u001b[0m             \u001b[39mraise\u001b[39;00m DistributionNotFound(req, requirers)\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=783'>784</a>\u001b[0m     to_activate\u001b[39m.\u001b[39mappend(dist)\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=784'>785</a>\u001b[0m \u001b[39mif\u001b[39;00m dist \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m req:\n\u001b[1;32m    <a href='file:///opt/python/3.9.5/lib/python3.9/site-packages/pkg_resources/__init__.py?line=785'>786</a>\u001b[0m     \u001b[39m# Oops, the \"best\" so far conflicts with a dependency\u001b[39;00m\n",
      "\u001b[0;31mDistributionNotFound\u001b[0m: The 'google-cloud-bigquery-storage' distribution was not found and is required by the application"
     ]
    }
   ],
   "source": [
    "!pip3 freeze > \"../requirements.txt\"\n",
    "#!pip3 install -r \"../requirements.txt\"  # giving some error\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import copy\n",
    "import copy\n",
    "import pickle\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500) \n",
    "\n",
    "print('Hello git')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff8604e",
   "metadata": {},
   "source": [
    "## Custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287aad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBQ_data(query_string):\n",
    "    client = bigquery.Client('turing-230020')\n",
    "    query = client.query(query_string)\n",
    "    results = query.result()\n",
    "    return results.to_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d0ac6a",
   "metadata": {},
   "source": [
    "## Download raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7287e688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " developer_detail Data shape is (49834, 56) and unique developers are 49834\n",
      " phase1_data Data shape is (50021, 66) and unique developers are 50021\n",
      " dev_availability Data shape is (49022, 5) and unique developers are 49022\n",
      " df_phase2 Data shape is (49680, 45) and unique developers are 49680\n",
      " nts Data shape is (48606, 2) and unique developers are 48606\n",
      " acc_lci Data shape is (43901, 4) and unique developers are 43901\n",
      " self_dec_skills Data shape is (49987, 4) and unique developers are 49987\n",
      " sns Data shape is (49278, 2) and unique developers are 49278\n",
      " ml_mcq Data shape is (49003, 11) and unique developers are 49003\n",
      " Global Data shape is (49680, 54) and unique developers are 49680\n",
      " Global Data shape is (49680, 59) and unique developers are 49680\n"
     ]
    }
   ],
   "source": [
    "stack_mcq_mapping_query = \"\"\"with stack as(\n",
    "SELECT stack_demand_id as stack_id, stack_demand_name as stack_name, \n",
    "count(distinct mcq_id) as num_mcq, \n",
    "count(distinct skill_id) as num_skill,\n",
    "string_agg(distinct cast(mcq_id as string), ',') as mcq_id, \n",
    "string_agg(distinct cast(skill_id as string), ',') as skill_id\n",
    "from `turing-230020.devdb_mirror.dv2_stack_demand_skills` \n",
    "left join `turing-230020.devdb_mirror.dv2_skill_mcq` using(skill_id) \n",
    "left join `turing-230020.devdb_mirror.dv2_stack_demand` using(stack_demand_id)\n",
    "where require=1 and mcq_id not in (107,140,142,237) -- and require_for_job=1 \n",
    "group by 1,2)\n",
    "\n",
    "SELECT * except(rn) from (SELECT *, row_number() over (partition by mcq_id order by stack_name desc) as rn\n",
    "from stack ) where rn=1 \n",
    "\"\"\"\n",
    "\n",
    "skill_mcq_mapping_query = \"\"\"with skill_mcq_mapping as(\n",
    "SELECT skill_name as skill_challenge_name, skill_id, mcq_id from(\n",
    "SELECT * , row_number() over (partition by mcq_id order by length(skill_name), skill_id) as rn from(\n",
    "SELECT skill_mcq.skill_id, base_skill.skill_name, \n",
    "string_agg(cast(skill_mcq.mcq_id as string), ',') mcq_id , \n",
    "from `turing-230020.devdb_mirror.dv2_skill_mcq` skill_mcq \n",
    "left join `turing-230020.devdb_mirror.base_all_skills_v4` base_skill on skill_mcq.skill_id=base_skill.id\n",
    "where skill_id in(\n",
    "select skill_id from `turing-230020.devdb_mirror.dv2_skill_mcq` GROUP by 1 having count(*)>1\n",
    ") GROUP by 1,2)) where rn=1\n",
    "\n",
    "UNION all\n",
    "\n",
    "SELECT challenge_name as skill_challenge_name,skill_id, mcq_id from(\n",
    "SELECT * , row_number() over (partition by mcq_id order by skill_id) as rn from(\n",
    "SELECT skill_mcq.skill_id, ch_name.challenge_name, \n",
    "string_agg(cast(skill_mcq.mcq_id as string), ',') mcq_id , \n",
    "from `turing-230020.devdb_mirror.dv2_skill_mcq` skill_mcq \n",
    "left join `turing-230020.devdb_mirror.base_all_skills_v4` base_skill on skill_mcq.skill_id=base_skill.id\n",
    "left join `turing-230020.devdb_mirror.dv2_challenge` ch_name on skill_mcq.mcq_id=ch_name.challenge_id\n",
    "where skill_id in(\n",
    "select skill_id from `turing-230020.devdb_mirror.dv2_skill_mcq` GROUP by 1 having count(*)<2\n",
    ") GROUP by 1,2)) where rn=1\n",
    ")\n",
    "\n",
    "SELECT * from skill_mcq_mapping\n",
    "\"\"\"\n",
    "\n",
    "skill = GBQ_data(skill_mcq_mapping_query)\n",
    "stack = GBQ_data(stack_mcq_mapping_query)\n",
    "\n",
    "skill['skill_challenge_name'] = skill['skill_challenge_name'].str.replace(' ','')\n",
    "skill.loc[skill['skill_challenge_name']=='Python', 'mcq_id'] = '211'  # 107 is deprecated and catered in ml_mcq\n",
    "skill.loc[skill['skill_challenge_name']=='Vue.js', 'mcq_id'] = '208'  # 140 is deprecated and catered in ml_mcq\n",
    "skill.loc[skill['skill_challenge_name']=='DevOps', 'mcq_id'] = '236'  # 142 has same name DevOps\n",
    "skill.loc[skill['skill_challenge_name']=='AmazonRedshift', 'mcq_id'] = '164,240' # 237 has same name \n",
    "skill['mcq_id_set'] = skill['mcq_id'].apply(lambda x : x.split(',',-1)).apply(lambda x : set(map(int,x)))\n",
    "stack['mcq_id_set'] = stack['mcq_id'].apply(lambda x : x.split(',',-1)).apply(lambda x : set(map(int,x)))\n",
    "\n",
    "developer_detail_query = \"\"\"\n",
    "SELECT country.country_group, dd.*, length(dd.resume_plain) characters_in_reume \n",
    "from  devdb_mirror.developer_detail dd left join \n",
    "analytics_views.country_information country on dd.country_id=country.country_id\n",
    "where user_id in (SELECT distinct(dev_id) from `analytics_views.phase2_step1_dev_aggregated`);\n",
    "\"\"\"\n",
    "developer_detail = GBQ_data(developer_detail_query)\n",
    "developer_detail.rename(columns = {'user_id':'dev_id'}, inplace=True)\n",
    "developer_detail['region'] = np.where(developer_detail['country_group']==\"Latin and South America\", 'LATAM', 'RoW')\n",
    "print(f\" developer_detail Data shape is {developer_detail.shape} and unique developers are {developer_detail['dev_id'].nunique()}\")\n",
    "\n",
    "phase1_data_query = \"\"\"\n",
    "SELECT * from `turing-230020.analytics_views.phase1_dev_level_data` \n",
    "where dev_id in (SELECT distinct(dev_id) from `analytics_views.phase2_step1_dev_aggregated`)\n",
    "\"\"\"\n",
    "phase1_data = GBQ_data(phase1_data_query)\n",
    "print(f\" phase1_data Data shape is {phase1_data.shape} and unique developers are {phase1_data['dev_id'].nunique()}\")\n",
    "\n",
    "dev_availability_query = \"\"\"\n",
    "SELECT user_id as dev_id, answer as latest_availability ,  action_from as updated_by,\n",
    "last_update as last_update_availability, notice_period\n",
    "from `turing-230020.devdb_mirror.dv2_developer_availability`\n",
    "where user_id in (SELECT distinct(dev_id) from `analytics_views.phase2_step1_dev_aggregated`)\n",
    "\"\"\"\n",
    "dev_availability = GBQ_data(dev_availability_query)\n",
    "print(f\" dev_availability Data shape is {dev_availability.shape} and unique developers are {dev_availability['dev_id'].nunique()}\")\n",
    "\n",
    "df_phase2_query = \"\"\"\n",
    "SELECT * FROM analytics_views.phase2_step1_dev_aggregated\n",
    "LEFT JOIN analytics_views.phase2_step2_dev_packet_aggre using(dev_id)\n",
    "LEFT JOIN analytics_views.phase2_step3_dev_interview_aggre using(dev_id)\n",
    "LEFT JOIN analytics_views.phase2_step4_dev_trials_and_engagements using (dev_id)\n",
    "where dev_id in (Select distinct dev_id from turing-dev-337819.pdsa.PDAS_P2_cluster where cluster is not null)\n",
    ";\"\"\"\n",
    "\n",
    "df_phase2 = GBQ_data(df_phase2_query)\n",
    "df_phase2['count_paying_cust'] = df_phase2['count_paying_cust'].fillna(0)\n",
    "df_phase2['paying_cust'] = np.where(df_phase2['count_paying_cust'] > 0, 1 , 0)\n",
    "print(f\" df_phase2 Data shape is {df_phase2.shape} and unique developers are {df_phase2['dev_id'].nunique()}\")\n",
    "\n",
    "nts_query = \"\"\"\n",
    "SELECT distinct developer_id as dev_id, 1 as NTE_status  \n",
    "FROM `turing-230020.devdb_mirror.ms2_negotiations`;\n",
    "\"\"\"\n",
    "nts = GBQ_data(nts_query)\n",
    "print(f\" nts Data shape is {nts.shape} and unique developers are {nts['dev_id'].nunique()}\")\n",
    "\n",
    "acc_lci_query = \"\"\"\n",
    "with lci as(\n",
    "SELECT user_id as lci_user_id, count(*) lci_attempts, max(total_score_by_cases) lci_score \n",
    "FROM `turing-230020.devdb_mirror.dv2_challenge_submit` \n",
    "where challenge_id=201 and user_id in (SELECT distinct(dev_id) from `analytics_views.phase2_step1_dev_aggregated`)\n",
    "group by 1),\n",
    "\n",
    "acc as(\n",
    "SELECT user_id as acc_user_id, count(*) acc_attempts, max(total_score_by_cases) acc_score\n",
    "FROM  `turing-230020.devdb_mirror.dv2_challenge_submit` AS dcs\n",
    "WHERE  challenge_id = 220 and user_id in (SELECT distinct(dev_id) from `analytics_views.phase2_step1_dev_aggregated`)\n",
    "GROUP by 1\n",
    "),\n",
    "\n",
    "acc_lci as(\n",
    "SELECT lci_user_id, acc_user_id,  COALESCE(lci_user_id, acc_user_id) user_id,\n",
    "acc_score, lci_score, COALESCE(acc_score, lci_score) as score\n",
    "from lci full outer join acc on lci.lci_user_id=acc.acc_user_id\n",
    ")\n",
    "\n",
    "SELECT user_id as dev_id, acc_score, lci_score, \n",
    "COALESCE(acc_score, lci_score) as acc_lci_score \n",
    "from acc_lci;\n",
    "\"\"\"\n",
    "acc_lci = GBQ_data(acc_lci_query)\n",
    "print(f\" acc_lci Data shape is {acc_lci.shape} and unique developers are {acc_lci['dev_id'].nunique()}\")\n",
    "\n",
    "self_dec_skills_query = \"\"\"\n",
    "SELECT developer_id as dev_id, count(distinct skill_id) num_skill, \n",
    "string_agg(distinct cast(skill_id as string), ',' ) as dev_skill_id,\n",
    "string_agg(distinct skill_name, ',' ) as dev_skill_name\n",
    "from(\n",
    "SELECT dev.*, skill.skill_name from `turing-230020.devdb_mirror.tpm_developer_skill` dev\n",
    "left join `turing-230020.devdb_mirror.base_all_skills_v4` skill on dev.skill_id = skill.id\n",
    "where developer_id in (SELECT distinct dev_id from `analytics_views.phase2_step1_dev_aggregated`)\n",
    ")\n",
    "GROUP by 1;\n",
    "\"\"\"\n",
    "self_dec_skills = GBQ_data(self_dec_skills_query)\n",
    "print(f\" self_dec_skills Data shape is {self_dec_skills.shape} and unique developers are {self_dec_skills['dev_id'].nunique()}\")\n",
    "\n",
    "sns_qeury = \"\"\"\n",
    "SELECT\n",
    "  dcs.user_id AS dev_id,\n",
    "  AVG(dweas.avg_score) AS sn_avg_score\n",
    "FROM\n",
    "  devdb_mirror.dv2_challenge_submit AS dcs\n",
    "  LEFT JOIN devdb_mirror.dv2_work_experience_avg_score AS dweas ON dcs.submit_id = dweas.submit_id\n",
    "  where user_id in (SELECT distinct(dev_id) from `analytics_views.phase2_step1_dev_aggregated`)\n",
    "GROUP BY\n",
    "  dcs.user_id\n",
    "\"\"\"\n",
    "sns = GBQ_data(sns_qeury)\n",
    "sns.rename(columns = {'sn_avg_score':'seniority_score'}, inplace=True)\n",
    "print(f\" sns Data shape is {sns.shape} and unique developers are {sns['dev_id'].nunique()}\")\n",
    "\n",
    "\n",
    "ml_mcq_query = \"\"\"with per as(\n",
    "select\n",
    "dms.dev_id, dms.challenge_name, dms.challenge_id, dms.problems_in_challenge, dms.num_attempted, dms.num_correct,\n",
    "dms.probability_correct, dms.dev_rank, dms.dev_percentile, dms.dev_weight, dms.last_updated_at, dms.from_model, \n",
    "dsm.skill_id, dsm.mcq_id, bas4.skill_name, dms.skill_id as dms_skill_id\n",
    "  from \n",
    "    external_query(\"turing-230020.us.machine-learning\",\n",
    "      \"select * from prod.dev_mcq_score\"\n",
    "    ) as dms\n",
    "  left join devdb_mirror.dv2_skill_mcq as dsm\n",
    "    on dms.challenge_id = dsm.mcq_id\n",
    "  left join devdb_mirror.base_all_skills_v4 as bas4\n",
    "    on dsm.skill_id = bas4.id where dev_id in (SELECT distinct(dev_id) from `analytics_views.phase2_step1_dev_aggregated`)\n",
    "    ),\n",
    "\n",
    "per_2 as(\n",
    "SELECT * except(challenge_id), \n",
    "case when challenge_id=142 then 236 when challenge_id=237 then 240 else challenge_id end as challenge_id\n",
    "from per), \n",
    "\n",
    "dev_mcq as (\n",
    "SELECT * except(rn) from (SELECT *, row_number() over (partition by dev_id, challenge_id order by dev_percentile desc, last_updated_at) as rn\n",
    "from per_2 ) where rn=1 \n",
    ")\n",
    "\n",
    "SELECT dev_id, \n",
    "count(distinct challenge_id) as num_challenges,\n",
    "string_agg(cast(challenge_id as string), ',') as challenge_ids,\n",
    "count(case when dev_percentile > 50 then challenge_id else null END) as passed_num_challenges ,\n",
    "string_agg(CASE WHEN dev_percentile > 50 then cast(challenge_id as string) else null END, ',') AS passed_challenge_ids,\n",
    "string_agg(CASE WHEN dev_percentile > 50 then challenge_name else null END, ',') AS passed_challenge_name,\n",
    "SUM(problems_in_challenge) as total_problems,\n",
    "sum(num_attempted) as attempted_problems,\n",
    "sum(num_correct) as num_correct,\n",
    "string_agg(cast(dev_percentile as string), ',') as dev_percentile,\n",
    "avg(dev_percentile) as mean_dev_percentile\n",
    "from dev_mcq WHERE challenge_id in (SELECT distinct mcq_id from `turing-230020.devdb_mirror.dv2_skill_mcq`) \n",
    "GROUP by 1\"\"\"  \n",
    "\n",
    "ml_mcq = GBQ_data(ml_mcq_query)\n",
    "print(f\" ml_mcq Data shape is {ml_mcq.shape} and unique developers are {ml_mcq['dev_id'].nunique()}\")\n",
    "ml_mcq_zero_passed = ml_mcq.loc[ml_mcq.passed_num_challenges ==0 , ]\n",
    "ml_mcq = ml_mcq.loc[ml_mcq.passed_num_challenges > 0 , ]\n",
    "ml_mcq['passed_challenge_ids_set'] = ml_mcq['passed_challenge_ids'].apply(lambda x : x.split(',',-1)).apply(lambda x : set(map(int,x)))\n",
    "ml_mcq['challenge_ids_att_set'] = ml_mcq['challenge_ids'].apply(lambda x : x.split(',',-1)).apply(lambda x : set(map(int,x)))\n",
    "\n",
    "ml_mcq['num_passed_stack'] = None\n",
    "ml_mcq['num_attempted_stack'] = None\n",
    "ml_mcq['num_passed_skill'] = None\n",
    "ml_mcq['num_attempted_skill'] = None\n",
    "\n",
    "ml_mcq['passed_skill_id'] = None\n",
    "ml_mcq['passed_skill_name'] = None\n",
    "ml_mcq['passed_stack_id'] = None\n",
    "ml_mcq['passed_stack_name'] = None\n",
    "\n",
    "\n",
    "for i in ml_mcq.index:\n",
    "    num_passed_stack = 0\n",
    "    num_attempted_stack = 0\n",
    "    num_passed_skill = 0\n",
    "    num_attempted_skill = 0\n",
    "\n",
    "    list_id = []\n",
    "    list_name = []\n",
    "    stack_id = []\n",
    "    stack_name = []\n",
    "\n",
    "    for k in stack.index:\n",
    "        if stack['mcq_id_set'][k].issubset(ml_mcq['passed_challenge_ids_set'][i]):\n",
    "            stack_id.append(stack['stack_id'][k])\n",
    "            stack_name.append(stack['stack_name'][k])\n",
    "            num_passed_stack = num_passed_stack+1\n",
    "        if stack['mcq_id_set'][k].issubset(ml_mcq['challenge_ids_att_set'][i]):\n",
    "            num_attempted_stack = num_attempted_stack+1 \n",
    "\n",
    "    #ml_mcq['num_passed_stack'][i] = num_passed_stack\n",
    "    ml_mcq.at[i, 'num_passed_stack'] = num_passed_stack\n",
    "    ml_mcq.at[i, 'num_attempted_stack'] = num_attempted_stack\n",
    "    ml_mcq.at[i, 'passed_stack_id'] = stack_id\n",
    "    ml_mcq.at[i, 'passed_stack_name'] = stack_name\n",
    "\n",
    "    for m in skill.index:\n",
    "        if skill['mcq_id_set'][m].issubset(ml_mcq['passed_challenge_ids_set'][i]):\n",
    "            num_passed_skill = num_passed_skill+1\n",
    "            list_id.append(skill['skill_id'][m])\n",
    "            list_name.append(skill['skill_challenge_name'][m])\n",
    "        if skill['mcq_id_set'][m].issubset(ml_mcq['challenge_ids_att_set'][i]):\n",
    "            num_attempted_skill = num_attempted_skill+1 \n",
    "\n",
    "    #ml_mcq['num_passed_skill'][i] = num_passed_skill\n",
    "    ml_mcq.at[i, 'num_passed_skill'] = num_passed_skill\n",
    "    ml_mcq.at[i, 'num_attempted_skill'] = num_attempted_skill\n",
    "    ml_mcq.at[i, 'passed_skill_id'] = list_id\n",
    "    ml_mcq.at[i, 'passed_skill_name'] = list_name\n",
    "\n",
    "#print(f\"ml_mcq shape {ml_mcq.shape}\")\n",
    "\n",
    "vetted_stack_skill = pd.concat([ml_mcq, ml_mcq_zero_passed])\n",
    "\n",
    "df_phase2_cols = ['dev_id', 'vetting_status', 'phase2_entry_date', 'phase2_entry_source',\n",
    "             'count_jobs_suggested', 'count_jobs_packets_sent', 'count_jobs_selected_for_interview',\n",
    "              'paying_cust','count_paying_cust', 'count_trials', 'count_engagements']\n",
    "\n",
    "acc_score_cols = ['dev_id', 'acc_score', 'lci_score', 'acc_lci_score']\n",
    "\n",
    "vetted_stack_skill_cols = ['dev_id', 'num_challenges', 'passed_num_challenges','total_problems','attempted_problems',\n",
    "                           'num_correct','mean_dev_percentile', 'num_passed_stack', 'passed_stack_id', 'passed_stack_name',\n",
    "                           'num_passed_skill','passed_skill_id', 'passed_skill_name', 'passed_challenge_ids_set',\n",
    "                           'challenge_ids_att_set']\n",
    "\n",
    "dev_detail_data_p1_cols = ['dev_id', 'signup_date', 'years_of_experience', 'words_in_resume',\n",
    "'resume_upload_date', 'Region', 'source_attribution_type', 'channel', 'english_communication',\n",
    "'full_name', 'email', 'hourly_rate']\n",
    "\n",
    "dev_detail_cols = ['dev_id', 'years_of_experience', 'characters_in_reume',\n",
    "                           'resume_upload_date', 'region', 'country', 'verbal_communication', 'hourly_rate']\n",
    "\n",
    "p1_data_cols = ['dev_id', 'full_name', 'email' , 'signup_date','source_attribution_type', 'channel', 'user_os', 'user_os_type',\n",
    "                           'quiz_answer', 'resume_flag', 'english_communication']\n",
    "\n",
    "\n",
    "dev_avaiability_cols = ['dev_id', 'latest_availability', 'updated_by', 'last_update_availability','notice_period']\n",
    "\n",
    "seniority_score_cols = ['dev_id', 'seniority_score']\n",
    "\n",
    "NTE_call_cols = ['dev_id', 'NTE_status']\n",
    "\n",
    "self_dec_skills_cols = ['dev_id', 'num_skill', 'dev_skill_id', 'dev_skill_name']\n",
    "\n",
    "\n",
    "\n",
    "global_data = pd.DataFrame()\n",
    "global_data = df_phase2[df_phase2_cols].merge(acc_lci[acc_score_cols], how='left', on='dev_id')\n",
    "global_data = global_data.merge(sns, how='left', on='dev_id')\n",
    "global_data = global_data.merge(developer_detail[dev_detail_cols], how='left', on='dev_id')\n",
    "global_data = global_data.merge(phase1_data[p1_data_cols], how='left', on='dev_id')\n",
    "global_data = global_data.merge(dev_availability, how='left', on='dev_id')\n",
    "global_data = global_data.merge(vetted_stack_skill[vetted_stack_skill_cols], how='left', on='dev_id')\n",
    "global_data = global_data.merge(nts, how='left', on='dev_id')\n",
    "global_data = global_data.merge(self_dec_skills[self_dec_skills_cols], how='left', on='dev_id')\n",
    "print(f\" Global Data shape is {global_data.shape} and unique developers are {global_data['dev_id'].nunique()}\")\n",
    "\n",
    "data_skill = global_data.loc[global_data['num_passed_skill']>0,].copy()\n",
    "\n",
    "data_skill['top_skill_demand'] = np.select(\n",
    "    [\n",
    "        data_skill['passed_skill_name'].apply(lambda x:True if 'React' in x else False),\n",
    "        data_skill['passed_skill_name'].apply(lambda x:True if 'NodeJS' in x else False),\n",
    "        data_skill['passed_skill_name'].apply(lambda x:True if 'Python' in x else False), \n",
    "        data_skill['passed_skill_name'].apply(lambda x:True if 'JavaScript' in x else False)\n",
    "    ], \n",
    "    [\n",
    "        'React', \n",
    "        'NodeJS',\n",
    "        'Python',\n",
    "        'JavaScript'\n",
    "    ], \n",
    "    default='Other'\n",
    ")\n",
    "\n",
    "data_skill['top_skill_supply'] = np.select(\n",
    "    [\n",
    "        data_skill['passed_skill_name'].apply(lambda x:True if 'SQL' in x else False),\n",
    "        data_skill['passed_skill_name'].apply(lambda x:True if 'Git' in x else False),\n",
    "        data_skill['passed_skill_name'].apply(lambda x:True if 'Java' in x else False), \n",
    "        data_skill['passed_skill_name'].apply(lambda x:True if 'NodeJS' in x else False)\n",
    "    ], \n",
    "    [\n",
    "        'SQL', \n",
    "        'Git',\n",
    "        'Java',\n",
    "        'NodeJS'\n",
    "    ], \n",
    "    default='Other'\n",
    ")\n",
    "\n",
    "data_stack = global_data.loc[global_data['num_passed_stack']>0,].copy()\n",
    "\n",
    "data_stack['top_stack_supply'] = np.select(\n",
    "    [\n",
    "        data_stack['passed_stack_name'].apply(lambda x:True if 'React Native' in x else False),\n",
    "        data_stack['passed_stack_name'].apply(lambda x:True if 'Python (Flask/Vue/Angular)' in x else False),\n",
    "        data_stack['passed_stack_name'].apply(lambda x:True if 'React + Backend' in x else False), \n",
    "        data_stack['passed_stack_name'].apply(lambda x:True if 'Android (Kotlin)' in x else False)\n",
    "    ], \n",
    "    [\n",
    "        'React Native', \n",
    "        'Python (Flask/Vue/Angular)/Backend',\n",
    "        'React + Backend/Backend',\n",
    "        'Android (Kotlin)'\n",
    "    ], \n",
    "    default='Other'\n",
    ")\n",
    "\n",
    "#print(data_skill['top_skill_supply'].value_counts())\n",
    "#print(data_skill['top_skill_demand'].value_counts())\n",
    "#print(data_stack['top_stack_supply'].value_counts())\n",
    "\n",
    "global_data = global_data.merge(data_skill[['dev_id','top_skill_supply','top_skill_demand']], how='left', on='dev_id')\n",
    "global_data = global_data.merge(data_stack[['dev_id','top_stack_supply']], how='left', on='dev_id')\n",
    "\n",
    "global_data.loc[global_data['passed_num_challenges']==0, 'num_passed_stack'] = 0\n",
    "global_data.loc[global_data['passed_num_challenges']==0, 'num_passed_skill'] = 0\n",
    "\n",
    "global_data.loc[global_data['passed_num_challenges']==0, 'top_skill_supply'] = 'No_skill_passed'\n",
    "global_data.loc[global_data['passed_num_challenges']==0, 'top_skill_demand'] = 'No_skill_passed'\n",
    "global_data.loc[global_data['passed_num_challenges']==0, 'top_stack_supply'] = 'No_stack_passed'\n",
    "\n",
    "global_data.loc[global_data['num_passed_stack']==0, 'top_stack_supply'] = 'No_stack_passed'\n",
    "global_data.loc[global_data['num_passed_skill']==0, 'top_skill_supply'] = 'No_skill_passed'\n",
    "global_data.loc[global_data['num_passed_skill']==0, 'top_skill_demand'] = 'No_skill_passed'\n",
    "\n",
    "global_data.loc[:,'correct_per_challenge'] = global_data.loc[:,'passed_num_challenges']/global_data.loc[:,'num_challenges']\n",
    "global_data.loc[:,'correct_per_questions'] =  global_data.loc[:,'num_correct']/global_data.loc[:,'total_problems']\n",
    "global_data['NTE_status'] = global_data['NTE_status'].fillna(0)\n",
    "print(f\" Global Data shape is {global_data.shape} and unique developers are {global_data['dev_id'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a02e5",
   "metadata": {},
   "source": [
    "## Store and version raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e480e0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Data of shape (49680, 59) stored in a csv successfully\n"
     ]
    }
   ],
   "source": [
    "if global_data['dev_id'].duplicated().any():\n",
    "    print('Global Data has duplicated dev_id')\n",
    "else:  \n",
    "    print(f'Global Data of shape {global_data.shape} stored in a csv successfully')\n",
    "    now = dt.datetime.now()\n",
    "    current_time = now.strftime(\"%d-%m-%y\") # %H:%M:%S\")\n",
    "    global_data.to_csv('../data/raw/' + '1.0-um-data-prep-all-' + current_time+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1246bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7598dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
